# Image Captioning with Robustness Analysis

## Team Information
- **Team Members**:  
  - Lovish Kaushik, lovishkaushik.24@kgpian.iitkgp.ac.in, 24AI91R05
  - Priansh Gangrade, GANGRADEPRIANSH07.24@kgpian.iitkgp.ac.in, 24CS60R13   
  - Dip Sambhavani, dip.sambhavani.24@kgpian.iitkgp.ac.in, 24CS60R45  

## Running Instructions

### Installation
To install the required libraries, run the following command:

```bash
pip install numpy pandas matplotlib Pillow tqdm evaluate torch transformers scikit-learn
```

### Dataset
Download the dataset from [this link](https://drive.google.com/file/d/1FMVcFM78XZE1KE1rlkGBpCdcdI58S1LB/view?usp=sharing) and extract it to your working directory.

### Steps to Run
1. **Training the Custom Model**:
   - Open the `team_id_30_a.ipynb` notebook.
   - Ensure the dataset paths are correctly set in the notebook.
   - Run all cells to train the custom encoder-decoder model.

2. **Generating Captions**:
   - Use `team_id_30_b.ipynb` for generating captions using both SmolVLM (zero-shot) and the custom model.
   - Update the image paths in the notebook to test with your images.

3. **Robustness Analysis**:
   - Run `team_id_30_b.ipynb` to evaluate the models on occluded images at different levels (10%, 50%, and 80%).

4. **Classifier Training**:
   - Use `team_id_30_c.ipynb` to train a BERT-based classifier for distinguishing between captions generated by SmolVLM and the custom model.

### Outputs
- Generated captions are saved in a CSV file for analysis.
- Evaluation scores (BLEU, METEOR, ROUGE-L) are displayed in the notebook outputs.