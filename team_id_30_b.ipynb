{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "529c363c",
   "metadata": {},
   "source": [
    "### **Loading ImageCaptioningModel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cd12ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    ViTModel,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    ViTImageProcessor,\n",
    "    AutoProcessor, \n",
    "    AutoModelForVision2Seq,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8543c5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b0c7a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, vit_name='WinKawaks/vit-small-patch16-224', gpt2_name='gpt2'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ViT encoder (drop classification head)\n",
    "        self.vit_processor = ViTImageProcessor.from_pretrained(vit_name)\n",
    "        self.vit_encoder = ViTModel.from_pretrained(vit_name)\n",
    "        \n",
    "        # GPT-2 decoder with cross-attention enabled\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token  # necessary for padding\n",
    "        self.gpt2_decoder = GPT2LMHeadModel.from_pretrained(gpt2_name, add_cross_attention=True)\n",
    "        \n",
    "        # Linear projector from ViT hidden size -> GPT-2 hidden size\n",
    "        self.vit_hidden_size = self.vit_encoder.config.hidden_size\n",
    "        self.gpt2_hidden_size = self.gpt2_decoder.config.hidden_size\n",
    "        self.projector = nn.Linear(self.vit_hidden_size, self.gpt2_hidden_size)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        # Step 1: Encode image with ViT\n",
    "        vit_inputs = self.vit_processor(images=images, return_tensors=\"pt\").to(self.vit_encoder.device)\n",
    "        vit_outputs = self.vit_encoder(**vit_inputs, output_hidden_states=False)\n",
    "        patch_embeddings = vit_outputs.last_hidden_state[:, 1:, :]  # remove CLS token\n",
    "\n",
    "        # Step 2: Project patch embeddings to GPT2 hidden size\n",
    "        projected_patches = self.projector(patch_embeddings)\n",
    "\n",
    "        # Step 3: Tokenize captions\n",
    "        caption_inputs = self.tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        input_ids = caption_inputs[\"input_ids\"].to(self.gpt2_decoder.device)\n",
    "        attention_mask = caption_inputs[\"attention_mask\"].to(self.gpt2_decoder.device)\n",
    "\n",
    "        # Step 4: Create attention mask for encoder (image patches)\n",
    "        encoder_attention_mask = torch.ones(projected_patches.shape[:2], dtype=torch.long).to(self.gpt2_decoder.device)\n",
    "\n",
    "        # Step 5: Decode with GPT2\n",
    "        outputs = self.gpt2_decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=projected_patches,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            labels=input_ids  # for training loss\n",
    "        )\n",
    "\n",
    "        return outputs  # contains loss and logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9768885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, load_path):\n",
    "    model = torch.load(load_path)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    print(f\"Model loaded from {load_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec2de84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = ImageCaptioningModel().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a14202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Later, reload the model\n",
    "model.load_state_dict(torch.load(\"/home/lovish/ImageCaptioning/Model/vit_gpt2_captioning_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1badf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, caption = self.data[idx]\n",
    "        image = image.convert(\"RGB\")  \n",
    "        return image, caption\n",
    "\n",
    "# Collate function for batching\n",
    "def collate_fn(batch):\n",
    "    images, captions = zip(*batch)\n",
    "    return list(images), list(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cd19367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption_from_image(img, model, max_length=50):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Convert image to RGB (in case it's not)\n",
    "    img = img.convert(\"RGB\")\n",
    "\n",
    "    # Step 1: Preprocess and encode image with ViT\n",
    "    vit_inputs = model.vit_processor(images=[img], return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        vit_outputs = model.vit_encoder(**vit_inputs)\n",
    "    patch_embeddings = vit_outputs.last_hidden_state[:, 1:, :]  # remove CLS\n",
    "    projected_patches = model.projector(patch_embeddings)\n",
    "\n",
    "    # Step 2: Generate caption from GPT-2\n",
    "    input_ids = torch.tensor([[model.tokenizer.eos_token_id]], device=device)  # Start token\n",
    "    encoder_attention_mask = torch.ones(projected_patches.shape[:-1], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            outputs = model.gpt2_decoder(\n",
    "                input_ids=input_ids,\n",
    "                encoder_hidden_states=projected_patches,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "            )\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "\n",
    "            # Append predicted token\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "\n",
    "            # Stop if EOS token is produced\n",
    "            if next_token.item() == model.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    # Decode to string, skipping the initial start token\n",
    "    caption = model.tokenizer.decode(input_ids[0][1:], skip_special_tokens=True)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcffe627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SmolVLM model and processor once\n",
    "smolvlm_processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
    "smolvlm_model = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolVLM-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    _attn_implementation=\"eager\"\n",
    ").to(DEVICE)\n",
    "\n",
    "smolvlm_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"give a detailed caption for this image\"}\n",
    "        ]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e49e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model_name, image, custom_model=None):\n",
    "    image = image.convert(\"RGB\")\n",
    "    \n",
    "    if model_name.lower() == \"smolvlm\":\n",
    "        prompt = smolvlm_processor.apply_chat_template(smolvlm_messages, add_generation_prompt=True)\n",
    "        inputs = smolvlm_processor(text=prompt, images=[image], return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            generated_ids = smolvlm_model.generate(**inputs, max_new_tokens=50)\n",
    "        generated_text = smolvlm_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        return generated_text.split(\"Assistant:\")[-1].strip()\n",
    "    \n",
    "    elif model_name.lower() == \"vit-gpt2\" and custom_model is not None:\n",
    "        return generate_caption_from_image(image, custom_model)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "425d6feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large, modern airport terminal building with a flat roof and a large glass facade. The building is made of light-colored stone and has a large, flat roof. The front of the building is open to the sky, and there are several large\n",
      " photo shows a truck and motorcycle parked in front of a yellow building. The truck has a yellow stripe on the front of it. There is a car behind the truck. There is a yellow stripe on the street. There is a white building behind the\n"
     ]
    }
   ],
   "source": [
    "# For SmolVLM\n",
    "caption = generate_caption(\"smolvlm\", Image.open(\"/home/lovish/ImageCaptioning/custom_captions_dataset/test/test_1.jpg\"))\n",
    "print(caption)\n",
    "\n",
    "# For ViT-GPT2\n",
    "caption = generate_caption(\"vit-gpt2\", Image.open(\"/home/lovish/ImageCaptioning/custom_captions_dataset/test/test_1.jpg\"), custom_model=model)\n",
    "print(caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2396143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(hypothesis,references):\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    rouge_score = rouge.compute(predictions=hypothesis,references=references)\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    BLEU_score = bleu.compute(predictions=hypothesis, references=references)\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "    meteor_score = meteor.compute(predictions=hypothesis,references=references)\n",
    "\n",
    "    return BLEU_score['bleu'], meteor_score['meteor'], rouge_score['rougeL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ae1df80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def occlude_image(img, occlusion_percent):\n",
    "    img = img.convert(\"RGB\").copy()\n",
    "    w, h = img.size\n",
    "    total_pixels = w * h\n",
    "    num_pixels_to_occlude = int(total_pixels * occlusion_percent)\n",
    "\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    while num_pixels_to_occlude > 0:\n",
    "        # Random block size and position\n",
    "        block_w = random.randint(10, 40)\n",
    "        block_h = random.randint(10, 40)\n",
    "        x = random.randint(0, w - block_w)\n",
    "        y = random.randint(0, h - block_h)\n",
    "\n",
    "        block_area = block_w * block_h\n",
    "        if block_area > num_pixels_to_occlude:\n",
    "            block_h = max(1, num_pixels_to_occlude // block_w)\n",
    "            block_area = block_w * block_h\n",
    "\n",
    "        draw.rectangle([x, y, x + block_w, y + block_h], fill=(0, 0, 0))\n",
    "        num_pixels_to_occlude -= block_area\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a199c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_on_occluded_image(data_loader, occlusion_levels, custom_model, device):\n",
    "    records = {\n",
    "        \"caption\": [],\n",
    "        \"reference\": [],\n",
    "        \"occlusion\": [],\n",
    "        \"model\": []\n",
    "    }\n",
    "\n",
    "    evaluation_scores = defaultdict(dict)\n",
    "\n",
    "    for occ in occlusion_levels:\n",
    "        for images, captions in tqdm(data_loader, desc=f\"Evaluating occlusion level {occ}\"):\n",
    "            for img, ref_caption in zip(images, captions):\n",
    "                occluded_img = occlude_image(img, occ)\n",
    "\n",
    "                # Evaluate with custom model\n",
    "                try:\n",
    "                    pred_custom = generate_caption(\"vit-gpt2\", occluded_img, custom_model=custom_model)\n",
    "                except Exception as e:\n",
    "                    pred_custom = f\"[ERROR: {e}]\"\n",
    "\n",
    "                records[\"caption\"].append(pred_custom)\n",
    "                records[\"reference\"].append(ref_caption)\n",
    "                records[\"occlusion\"].append(occ)\n",
    "                records[\"model\"].append(\"vit-gpt2\")\n",
    "\n",
    "                # Evaluate with smolvlm\n",
    "                try:\n",
    "                    pred_smol = generate_caption(\"smolvlm\", occluded_img)\n",
    "                except Exception as e:\n",
    "                    pred_smol = f\"[ERROR: {e}]\"\n",
    "\n",
    "                records[\"caption\"].append(pred_smol)\n",
    "                records[\"reference\"].append(ref_caption)\n",
    "                records[\"occlusion\"].append(occ)\n",
    "                records[\"model\"].append(\"smolvlm\")\n",
    "\n",
    "        # Compute scores for each model at current occlusion level\n",
    "        for model_name in [\"vit-gpt2\", \"smolvlm\"]:\n",
    "            preds = [c for c, m, o in zip(records[\"caption\"], records[\"model\"], records[\"occlusion\"]) if m == model_name and o == occ]\n",
    "            refs  = [r for r, m, o in zip(records[\"reference\"], records[\"model\"], records[\"occlusion\"]) if m == model_name and o == occ]\n",
    "\n",
    "            try:\n",
    "                bleu, meteor, rougeL = calculate_scores(preds, refs)\n",
    "                evaluation_scores[model_name][occ] = {\n",
    "                    \"BLEU\": bleu,\n",
    "                    \"METEOR\": meteor,\n",
    "                    \"ROUGE-L\": rougeL\n",
    "                }\n",
    "            except Exception as e:\n",
    "                evaluation_scores[model_name][occ] = {\n",
    "                    \"BLEU\": None,\n",
    "                    \"METEOR\": None,\n",
    "                    \"ROUGE-L\": None,\n",
    "                    \"Error\": str(e)\n",
    "                }\n",
    "\n",
    "    return {\n",
    "        \"records\": records,\n",
    "        \"scores\": evaluation_scores\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba404ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creatDataSet(csv_file_path,image_folderpath):\n",
    "    df=pd.read_csv(csv_file_path)\n",
    "    data=[]\n",
    "    for _ , row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        img=Image.open(f'{image_folderpath}/{row[\"filename\"]}')\n",
    "        gt_caption=row['caption']\n",
    "        data.append([img,gt_caption])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80720b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 928/928 [00:00<00:00, 4518.90it/s]\n"
     ]
    }
   ],
   "source": [
    "test_csv_path=\"custom_captions_dataset/test.csv\"\n",
    "test_image_path=\"custom_captions_dataset/test\"\n",
    "test_data=creatDataSet(test_csv_path,test_image_path)\n",
    "\n",
    "test_data = ImageCaptionDataset(test_data)\n",
    "test_loader = DataLoader(test_data, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87dbf6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating occlusion level 0.0: 100%|██████████| 116/116 [42:42<00:00, 22.09s/it]\n",
      "[nltk_data] Downloading package wordnet to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Evaluating occlusion level 0.1: 100%|██████████| 116/116 [40:51<00:00, 21.14s/it]\n",
      "[nltk_data] Downloading package wordnet to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Evaluating occlusion level 0.5: 100%|██████████| 116/116 [38:28<00:00, 19.90s/it]\n",
      "[nltk_data] Downloading package wordnet to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Evaluating occlusion level 0.8: 100%|██████████| 116/116 [37:29<00:00, 19.39s/it]\n",
      "[nltk_data] Downloading package wordnet to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             caption  \\\n",
      "0   image is of a bird eating a grass-fed chicken...   \n",
      "1  Two birds are standing on a paved path. The bi...   \n",
      "2  planes are flying in the sky. The sky is very ...   \n",
      "3  Four blue jets are flying in formation. The je...   \n",
      "4   bedroom has a large bed in it. The bed has wh...   \n",
      "\n",
      "                                           reference  occlusion     model  \n",
      "0  Two birds stand next to each other.  The birds...        0.0  vit-gpt2  \n",
      "1  Two birds stand next to each other.  The birds...        0.0   smolvlm  \n",
      "2  There are four jets flying in the sky together...        0.0  vit-gpt2  \n",
      "3  There are four jets flying in the sky together...        0.0   smolvlm  \n",
      "4  This is a hotel room. There is a patterned com...        0.0  vit-gpt2  \n",
      "defaultdict(<class 'dict'>, {'vit-gpt2': {0.0: {'BLEU': 0.06731295454548539, 'METEOR': np.float64(0.24733249028518053), 'ROUGE-L': np.float64(0.27065165815107983)}, 0.1: {'BLEU': 0.06234001328728869, 'METEOR': np.float64(0.24059009158816858), 'ROUGE-L': np.float64(0.26280973035257105)}, 0.5: {'BLEU': 0.05303069021522836, 'METEOR': np.float64(0.2192008985726052), 'ROUGE-L': np.float64(0.24259449068736796)}, 0.8: {'BLEU': 0.04395797020110086, 'METEOR': np.float64(0.20607194433245668), 'ROUGE-L': np.float64(0.22467066258485577)}}, 'smolvlm': {0.0: {'BLEU': 0.06339899968180299, 'METEOR': np.float64(0.23436718301270482), 'ROUGE-L': np.float64(0.2742979380390852)}, 0.1: {'BLEU': 0.06041942151346876, 'METEOR': np.float64(0.2307815531861651), 'ROUGE-L': np.float64(0.2724314592719359)}, 0.5: {'BLEU': 0.04050770229908609, 'METEOR': np.float64(0.19682980618717053), 'ROUGE-L': np.float64(0.24458888476740562)}, 0.8: {'BLEU': 0.025632136966549667, 'METEOR': np.float64(0.1600829049977319), 'ROUGE-L': np.float64(0.2174515947359012)}}})\n"
     ]
    }
   ],
   "source": [
    "# After running evaluate_on_occluded_image(...)\n",
    "result = evaluate_on_occluded_image(test_loader, [0.0, 0.1, 0.5, 0.8], model, DEVICE)\n",
    "\n",
    "# Create a DataFrame from records\n",
    "df = pd.DataFrame(result[\"records\"])\n",
    "df.to_csv('occluded_image_captions.csv')\n",
    "print(df.head())\n",
    "\n",
    "# Evaluation scores\n",
    "print(result[\"scores\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2dcffc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f99734cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>, {'vit-gpt2': {0.0: {'BLEU': 0.06731295454548539, 'METEOR': np.float64(0.24733249028518053), 'ROUGE-L': np.float64(0.27065165815107983)}, 0.1: {'BLEU': 0.06234001328728869, 'METEOR': np.float64(0.24059009158816858), 'ROUGE-L': np.float64(0.26280973035257105)}, 0.5: {'BLEU': 0.05303069021522836, 'METEOR': np.float64(0.2192008985726052), 'ROUGE-L': np.float64(0.24259449068736796)}, 0.8: {'BLEU': 0.04395797020110086, 'METEOR': np.float64(0.20607194433245668), 'ROUGE-L': np.float64(0.22467066258485577)}}, 'smolvlm': {0.0: {'BLEU': 0.06339899968180299, 'METEOR': np.float64(0.23436718301270482), 'ROUGE-L': np.float64(0.2742979380390852)}, 0.1: {'BLEU': 0.06041942151346876, 'METEOR': np.float64(0.2307815531861651), 'ROUGE-L': np.float64(0.2724314592719359)}, 0.5: {'BLEU': 0.04050770229908609, 'METEOR': np.float64(0.19682980618717053), 'ROUGE-L': np.float64(0.24458888476740562)}, 0.8: {'BLEU': 0.025632136966549667, 'METEOR': np.float64(0.1600829049977319), 'ROUGE-L': np.float64(0.2174515947359012)}}})\n"
     ]
    }
   ],
   "source": [
    "print(result[\"scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34dacf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
