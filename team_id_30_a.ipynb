{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff4fecc0",
   "metadata": {},
   "source": [
    "### **IMPORTs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1fe99e16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T07:48:25.108880Z",
     "iopub.status.busy": "2025-04-14T07:48:25.108072Z",
     "iopub.status.idle": "2025-04-14T07:48:26.184150Z",
     "shell.execute_reply": "2025-04-14T07:48:26.183603Z",
     "shell.execute_reply.started": "2025-04-14T07:48:25.108853Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    ViTModel,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    ViTImageProcessor,\n",
    "    AutoProcessor, \n",
    "    AutoModelForVision2Seq,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "392e0de7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T06:21:42.903282Z",
     "iopub.status.busy": "2025-04-14T06:21:42.902964Z",
     "iopub.status.idle": "2025-04-14T06:21:42.996692Z",
     "shell.execute_reply": "2025-04-14T06:21:42.995896Z",
     "shell.execute_reply.started": "2025-04-14T06:21:42.903254Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e50164",
   "metadata": {},
   "source": [
    "### **LOAD DATASETs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "970e95ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T06:22:35.810758Z",
     "iopub.status.busy": "2025-04-14T06:22:35.810220Z",
     "iopub.status.idle": "2025-04-14T06:22:35.814888Z",
     "shell.execute_reply": "2025-04-14T06:22:35.814199Z",
     "shell.execute_reply.started": "2025-04-14T06:22:35.810734Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def creatDataSet(csv_file_path,image_folderpath):\n",
    "    df=pd.read_csv(csv_file_path)\n",
    "    data=[]\n",
    "    for _ , row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        img=Image.open(f'{image_folderpath}/{row[\"filename\"]}')\n",
    "        gt_caption=row['caption']\n",
    "        data.append([img,gt_caption])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9def5a42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T06:23:46.057017Z",
     "iopub.status.busy": "2025-04-14T06:23:46.056469Z",
     "iopub.status.idle": "2025-04-14T06:24:44.493686Z",
     "shell.execute_reply": "2025-04-14T06:24:44.492876Z",
     "shell.execute_reply.started": "2025-04-14T06:23:46.056992Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5715 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5715/5715 [00:00<00:00, 5999.20it/s]\n"
     ]
    }
   ],
   "source": [
    "train_csv_path=\"/home/lovish/ImageCaptioning/custom_captions_dataset/train.csv\"\n",
    "train_image_path=\"custom_captions_dataset/train\"\n",
    "train_data=creatDataSet(train_csv_path,train_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5aac8c85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T06:24:44.495062Z",
     "iopub.status.busy": "2025-04-14T06:24:44.494811Z",
     "iopub.status.idle": "2025-04-14T06:24:54.530947Z",
     "shell.execute_reply": "2025-04-14T06:24:54.530391Z",
     "shell.execute_reply.started": "2025-04-14T06:24:44.495045Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 946/946 [00:00<00:00, 5923.73it/s]\n"
     ]
    }
   ],
   "source": [
    "val_csv_path=\"custom_captions_dataset/val.csv\"\n",
    "val_image_path=\"custom_captions_dataset/val\"\n",
    "val_data=creatDataSet(val_csv_path,val_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8b7955c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T06:24:54.531858Z",
     "iopub.status.busy": "2025-04-14T06:24:54.531626Z",
     "iopub.status.idle": "2025-04-14T06:25:04.111173Z",
     "shell.execute_reply": "2025-04-14T06:25:04.110562Z",
     "shell.execute_reply.started": "2025-04-14T06:24:54.531833Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 928/928 [00:00<00:00, 13031.02it/s]\n"
     ]
    }
   ],
   "source": [
    "test_csv_path=\"custom_captions_dataset/test.csv\"\n",
    "test_image_path=\"custom_captions_dataset/test\"\n",
    "test_data=creatDataSet(test_csv_path,test_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "563ab710",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T06:25:04.112899Z",
     "iopub.status.busy": "2025-04-14T06:25:04.112688Z",
     "iopub.status.idle": "2025-04-14T06:25:04.116739Z",
     "shell.execute_reply": "2025-04-14T06:25:04.116004Z",
     "shell.execute_reply.started": "2025-04-14T06:25:04.112882Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training datset : 5715\n",
      "Validation datset : 946\n",
      "Testing datset : 928\n"
     ]
    }
   ],
   "source": [
    "print(\"Training datset :\",len(train_data))\n",
    "print(\"Validation datset :\",len(val_data))\n",
    "print(\"Testing datset :\",len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee043e2",
   "metadata": {},
   "source": [
    "### **Zero Shot Caption Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "de7816b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T07:48:44.361522Z",
     "iopub.status.busy": "2025-04-14T07:48:44.361227Z",
     "iopub.status.idle": "2025-04-14T07:48:48.073051Z",
     "shell.execute_reply": "2025-04-14T07:48:48.072474Z",
     "shell.execute_reply.started": "2025-04-14T07:48:44.361501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# preloading smolvlm model\n",
    "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
    "smol_model = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolVLM-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    _attn_implementation=\"eager\"  # force eager mode\n",
    ").to(DEVICE)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\"\n",
    "            },\n",
    "            {\n",
    "                 \"type\": \"text\", \n",
    "                 \"text\": \"give a detailed caption for this image\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6ec0adfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T06:25:34.785089Z",
     "iopub.status.busy": "2025-04-14T06:25:34.784878Z",
     "iopub.status.idle": "2025-04-14T06:25:34.790009Z",
     "shell.execute_reply": "2025-04-14T06:25:34.789327Z",
     "shell.execute_reply.started": "2025-04-14T06:25:34.785073Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def zero_shot_captioning(img_path, model_name):\n",
    "    img = Image.open(img_path)\n",
    "    if model_name.lower() == 'smolvlm':\n",
    "        prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        inputs = processor(text=prompt, images=[img], return_tensors=\"pt\")\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        \n",
    "        generated_ids = smol_model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "        generated_texts = processor.batch_decode(\n",
    "            generated_ids,\n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "        return generated_texts[0].split('Assistant:')[1]\n",
    "    else :\n",
    "        print(f'{model_name} is currently not available in the program')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6f27452d-190f-469d-89d2-4909bf689bc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T06:28:02.569236Z",
     "iopub.status.busy": "2025-04-14T06:28:02.568551Z",
     "iopub.status.idle": "2025-04-14T06:28:06.891781Z",
     "shell.execute_reply": "2025-04-14T06:28:06.891170Z",
     "shell.execute_reply.started": "2025-04-14T06:28:02.569212Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A large, modern airport terminal building with a flat roof and a large glass facade. The building is made of light-colored stone and has a large, flat roof. The front of the building is open to the sky, and there are several large'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_captioning('custom_captions_dataset/test/test_1.jpg', 'smolvlm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f526a079-9337-4649-9273-2f8ee3e411a8",
   "metadata": {},
   "source": [
    "### **Custom ImageCaptionModel Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "eaf0d19a-5c8e-4188-ad2f-d8afd1c56dbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T06:51:15.609206Z",
     "iopub.status.busy": "2025-04-14T06:51:15.608951Z",
     "iopub.status.idle": "2025-04-14T06:51:15.616102Z",
     "shell.execute_reply": "2025-04-14T06:51:15.615463Z",
     "shell.execute_reply.started": "2025-04-14T06:51:15.609188Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, vit_name='WinKawaks/vit-small-patch16-224', gpt2_name='gpt2'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ViT encoder (drop classification head)\n",
    "        self.vit_processor = ViTImageProcessor.from_pretrained(vit_name)\n",
    "        self.vit_encoder = ViTModel.from_pretrained(vit_name)\n",
    "        \n",
    "        # GPT-2 decoder with cross-attention enabled\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token  # necessary for padding\n",
    "        self.gpt2_decoder = GPT2LMHeadModel.from_pretrained(gpt2_name, add_cross_attention=True)\n",
    "        \n",
    "        # Linear projector from ViT hidden size -> GPT-2 hidden size\n",
    "        self.vit_hidden_size = self.vit_encoder.config.hidden_size\n",
    "        self.gpt2_hidden_size = self.gpt2_decoder.config.hidden_size\n",
    "        self.projector = nn.Linear(self.vit_hidden_size, self.gpt2_hidden_size)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        # Step 1: Encode image with ViT\n",
    "        vit_inputs = self.vit_processor(images=images, return_tensors=\"pt\").to(self.vit_encoder.device)\n",
    "        vit_outputs = self.vit_encoder(**vit_inputs, output_hidden_states=False)\n",
    "        patch_embeddings = vit_outputs.last_hidden_state[:, 1:, :]  # remove CLS token\n",
    "\n",
    "        # Step 2: Project patch embeddings to GPT2 hidden size\n",
    "        projected_patches = self.projector(patch_embeddings)\n",
    "\n",
    "        # Step 3: Tokenize captions\n",
    "        caption_inputs = self.tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        input_ids = caption_inputs[\"input_ids\"].to(self.gpt2_decoder.device)\n",
    "        attention_mask = caption_inputs[\"attention_mask\"].to(self.gpt2_decoder.device)\n",
    "\n",
    "        # Step 4: Create attention mask for encoder (image patches)\n",
    "        encoder_attention_mask = torch.ones(projected_patches.shape[:2], dtype=torch.long).to(self.gpt2_decoder.device)\n",
    "\n",
    "        # Step 5: Decode with GPT2\n",
    "        outputs = self.gpt2_decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=projected_patches,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            labels=input_ids  # for training loss\n",
    "        )\n",
    "\n",
    "        return outputs  # contains loss and logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "22eb29ce-593a-4659-9bc8-49fbb38c4be8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-14T07:06:51.745723Z",
     "iopub.status.idle": "2025-04-14T07:06:51.745925Z",
     "shell.execute_reply": "2025-04-14T07:06:51.745835Z",
     "shell.execute_reply.started": "2025-04-14T07:06:51.745826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, caption = self.data[idx]\n",
    "        image = image.convert(\"RGB\")  \n",
    "        return image, caption\n",
    "\n",
    "# Collate function for batching\n",
    "def collate_fn(batch):\n",
    "    images, captions = zip(*batch)\n",
    "    return list(images), list(captions)\n",
    "\n",
    "# Save model weights\n",
    "def save_model(model, save_path):\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "97904f2a-25d7-45fd-9691-61e883cc8cde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T06:50:24.923719Z",
     "iopub.status.busy": "2025-04-14T06:50:24.923404Z",
     "iopub.status.idle": "2025-04-14T06:50:24.929089Z",
     "shell.execute_reply": "2025-04-14T06:50:24.928386Z",
     "shell.execute_reply.started": "2025-04-14T06:50:24.923699Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, device, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "        for images, captions in loop:\n",
    "            # Forward pass\n",
    "            outputs = model(images=images, captions=captions)\n",
    "            loss = outputs.loss\n",
    "    \n",
    "            # Backward + optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "        print(f\"Epoch {epoch+1} average loss: {total_loss / len(train_loader):.4f}\")\n",
    "    save_path = \"model/vit_gpt2_captioning_model.pth\"\n",
    "    save_model(model, save_path)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e7fcf03a-9dce-4ad8-8e87-50db55fbe3ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T06:53:06.513786Z",
     "iopub.status.busy": "2025-04-14T06:53:06.512974Z",
     "iopub.status.idle": "2025-04-14T06:53:07.915388Z",
     "shell.execute_reply": "2025-04-14T06:53:07.914625Z",
     "shell.execute_reply.started": "2025-04-14T06:53:06.513761Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-small-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = ImageCaptioningModel().to(DEVICE)\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataset = ImageCaptionDataset(train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c078d0fb-6a39-4ea2-8110-fd8f3fd2c3bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T06:55:25.872704Z",
     "iopub.status.busy": "2025-04-14T06:55:25.872175Z",
     "iopub.status.idle": "2025-04-14T07:06:51.721400Z",
     "shell.execute_reply": "2025-04-14T07:06:51.720759Z",
     "shell.execute_reply.started": "2025-04-14T06:55:25.872681Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/715 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Epoch 1/3: 100%|██████████| 715/715 [04:00<00:00,  2.97it/s, loss=1.72] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average loss: 1.7176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 715/715 [03:42<00:00,  3.21it/s, loss=1.24] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 average loss: 1.5278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 715/715 [03:42<00:00,  3.21it/s, loss=1.47] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 average loss: 1.4155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_model(model, train_loader, optimizer, DEVICE, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf47073",
   "metadata": {},
   "source": [
    "**Loading saved model weights (ran 10 epochs)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e56763e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Later, reload the model\n",
    "model.load_state_dict(torch.load(\"/home/lovish/ImageCaptioning/Model/vit_gpt2_captioning_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a6ba9eab-1d34-4f8c-a861-78637a54fabc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T07:19:35.229448Z",
     "iopub.status.busy": "2025-04-14T07:19:35.229179Z",
     "iopub.status.idle": "2025-04-14T07:19:35.236832Z",
     "shell.execute_reply": "2025-04-14T07:19:35.236145Z",
     "shell.execute_reply.started": "2025-04-14T07:19:35.229431Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_caption_from_image(img, model, max_length=50):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Convert image to RGB (in case it's not)\n",
    "    img = img.convert(\"RGB\")\n",
    "\n",
    "    # Step 1: Preprocess and encode image with ViT\n",
    "    vit_inputs = model.vit_processor(images=[img], return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        vit_outputs = model.vit_encoder(**vit_inputs)\n",
    "    patch_embeddings = vit_outputs.last_hidden_state[:, 1:, :]  # remove CLS\n",
    "    projected_patches = model.projector(patch_embeddings)\n",
    "\n",
    "    # Step 2: Generate caption from GPT-2\n",
    "    input_ids = torch.tensor([[model.tokenizer.eos_token_id]], device=device)  # Start token\n",
    "    encoder_attention_mask = torch.ones(projected_patches.shape[:-1], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            outputs = model.gpt2_decoder(\n",
    "                input_ids=input_ids,\n",
    "                encoder_hidden_states=projected_patches,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "            )\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "\n",
    "            # Append predicted token\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "\n",
    "            # Stop if EOS token is produced\n",
    "            if next_token.item() == model.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    # Decode to string, skipping the initial start token\n",
    "    caption = model.tokenizer.decode(input_ids[0][1:], skip_special_tokens=True)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4509918b-d9dd-4ecf-addb-dacd9d68550d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T07:19:52.636383Z",
     "iopub.status.busy": "2025-04-14T07:19:52.635742Z",
     "iopub.status.idle": "2025-04-14T07:19:52.641983Z",
     "shell.execute_reply": "2025-04-14T07:19:52.641220Z",
     "shell.execute_reply.started": "2025-04-14T07:19:52.636363Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_scores(hypothesis,references):\n",
    "    \n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    rouge_score = rouge.compute(predictions=hypothesis,references=references)\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    BLEU_score = bleu.compute(predictions=hypothesis, references=references)\n",
    "    meteor = evaluate.load(\"meteor\")\n",
    "    meteor_score = meteor.compute(predictions=hypothesis,references=references)\n",
    "\n",
    "    return BLEU_score['bleu'], meteor_score['meteor'], rouge_score['rougeL']\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "\n",
    "    for images, captions in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "        for img, ref_caption in zip(images, captions):\n",
    "            prediction = generate_caption_from_image(img, model)\n",
    "            all_predictions.append(prediction)\n",
    "            all_references.append(ref_caption)\n",
    "\n",
    "    # Compute evaluation scores\n",
    "    bleu, meteor, rougeL = calculate_scores(all_predictions, all_references)\n",
    "\n",
    "    return {\n",
    "        \"BLEU\": bleu,\n",
    "        \"METEOR\": meteor,\n",
    "        \"ROUGE-L\": rougeL\n",
    "    }\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e013d33",
   "metadata": {},
   "source": [
    "### EVALUATING SCORES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b941c179",
   "metadata": {},
   "source": [
    "##### OUR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e73e1b-c69b-4076-a75a-84f41e9204b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T07:20:01.407881Z",
     "iopub.status.busy": "2025-04-14T07:20:01.407613Z",
     "iopub.status.idle": "2025-04-14T07:30:11.987174Z",
     "shell.execute_reply": "2025-04-14T07:30:11.986175Z",
     "shell.execute_reply.started": "2025-04-14T07:20:01.407860Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare DataLoader\n",
    "test_dataset = ImageCaptionDataset(test_data)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "results = evaluate_model(model, test_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "15b0bc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ OUR COUSTOM MODEL SCORES ____________________\n",
      "BLEU SCORE: 0.06731295454548539\n",
      "METEOR SCORE: 0.24733249028518053\n",
      "ROUGE SCORE: 0.2705377493522648\n"
     ]
    }
   ],
   "source": [
    "OurModelScores=results\n",
    "avgBLEU_OurModel=OurModelScores[\"BLEU\"]\n",
    "avgMETEOR_OurModel=OurModelScores[\"METEOR\"]\n",
    "avgROUGE_OurModel=OurModelScores[\"ROUGE-L\"]\n",
    "\n",
    "print(\"_\"*20,\"OUR COUSTOM MODEL SCORES\",\"_\"*20)\n",
    "print(\"BLEU SCORE:\",avgBLEU_OurModel)\n",
    "print(\"METEOR SCORE:\",avgMETEOR_OurModel)\n",
    "print(\"ROUGE SCORE:\",avgROUGE_OurModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ecef23",
   "metadata": {},
   "source": [
    "#### smolVLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f25bb9-5e2b-4c26-9f26-9b7b208e8d32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T07:49:19.285772Z",
     "iopub.status.busy": "2025-04-14T07:49:19.285519Z",
     "iopub.status.idle": "2025-04-14T07:49:19.291907Z",
     "shell.execute_reply": "2025-04-14T07:49:19.291134Z",
     "shell.execute_reply.started": "2025-04-14T07:49:19.285755Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_smolvlm(model, processor, data_loader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "\n",
    "    for images, captions in tqdm(data_loader, desc=\"Evaluating SmolVLM\"):\n",
    "        for img, ref_caption in zip(images, captions):\n",
    "            img = img.convert(\"RGB\")  # Ensure correct format\n",
    "            try:\n",
    "                prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "                inputs = processor(text=prompt, images=[img], return_tensors=\"pt\").to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    generated_ids = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "                generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "                predicted_caption = generated_text.split(\"Assistant:\")[-1].strip()\n",
    "\n",
    "                all_predictions.append(predicted_caption)\n",
    "                all_references.append(ref_caption)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Compute evaluation scores\n",
    "    bleu, meteor, rougeL = calculate_scores(all_predictions, all_references)\n",
    "\n",
    "    return {\n",
    "        \"BLEU\": bleu,\n",
    "        \"METEOR\": meteor,\n",
    "        \"ROUGE-L\": rougeL\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8c673564-2cc5-420f-b77a-3672ef5d335c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T07:50:00.363544Z",
     "iopub.status.busy": "2025-04-14T07:50:00.362848Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating SmolVLM: 100%|██████████| 116/116 [35:24<00:00, 18.32s/it]\n",
      "[nltk_data] Downloading package wordnet to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/lovish/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "smolVLM_scores=evaluate_smolvlm(smol_model, processor, test_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2f3b3651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ smallVLM MODEL SCORES ____________________\n",
      "BLEU SCORE: 0.06731295454548539\n",
      "METEOR SCORE: 0.23436718301270482\n",
      "ROUGE SCORE: 0.27432802118733357\n"
     ]
    }
   ],
   "source": [
    "\n",
    "avgBLEU_smolVLM=smolVLM_scores[\"BLEU\"]\n",
    "avgMETEOR_smolVLM=smolVLM_scores[\"METEOR\"]\n",
    "avgROUGE_smolVLM=smolVLM_scores[\"ROUGE-L\"]\n",
    "\n",
    "print(\"_\"*20,\"smallVLM MODEL SCORES\",\"_\"*20)\n",
    "print(\"BLEU SCORE:\",avgBLEU_OurModel)\n",
    "print(\"METEOR SCORE:\",avgMETEOR_smolVLM)\n",
    "print(\"ROUGE SCORE:\",avgROUGE_smolVLM)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7123633,
     "sourceId": 11377917,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
